# LLM_RAG_Model_FastAPI

Steps:

Creating a virtual environment managing the dependencies.
what .env file and and how to load secrets from .env file
how to configure and load llm models from local folder and using together api
how to modularize your code and create a vectore DB
Pydantic & What is fast api from concepts to code
What is gradio and how to create UIs using Gradio
Combine everything and create a fully functional LLM App.


python3 -m venv .venv source .venv/bin/activate which python python3 -m pip install --upgrade pip

model_path = https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
Kaggle Datatset link = https://www.kaggle.com/datasets/harshsinghal/nlp-and-llm-related-arxiv-papers
Embedding Model: https://huggingface.co/BAAI/bge-base-en-v1.5
Hugging Face(ChatInterface) -> https://www.gradio.app/docs/chatinterface
Steps:
